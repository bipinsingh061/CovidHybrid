{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import shuffle\n",
    "import cv2\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as gb\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D,GlobalMaxPooling2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import categorical_crossentropy\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH=224\n",
    "HEIGHT=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train\n",
      "dataset/test\n"
     ]
    }
   ],
   "source": [
    "train_image = \"dataset/train\"\n",
    "test_image = \"dataset/test\"\n",
    "\n",
    "\n",
    "print(train_image)\n",
    "print(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   rotation_range=15,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 3, 3, 2048)   0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18432)        0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          9437696     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            771         dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,157,507\n",
      "Trainable params: 9,569,795\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet50 = ResNet50(input_shape=(WIDTH,HEIGHT,3), weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "x = resnet50.output\n",
    "x = AveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=resnet50.input, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def precision_threshold(threshold = 0.5):\n",
    "    def precision(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(y_pred)\n",
    "        precision_ratio = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision_ratio\n",
    "    return precision\n",
    "def recall_threshold(threshold = 0.5):\n",
    "    def recall(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.clip(y_true, 0, 1))\n",
    "        recall_ratio = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall_ratio\n",
    "    return recall\n",
    "def fbeta_score_threshold(beta = 1, threshold = 0.5):\n",
    "    def fbeta_score(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        beta_value = beta\n",
    "        p = precision_threshold(threshold_value)(y_true, y_pred)\n",
    "        r = recall_threshold(threshold_value)(y_true, y_pred)\n",
    "        bb = beta_value ** 2\n",
    "        fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "        return fbeta_score\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5144 images belonging to 3 classes.\n",
      "Found 1288 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.8797 - precision: 0.6564 - recall: 0.6167 - fbeta_score: 0.6467 - accuracy: 0.6476 - auc: 0.7969 - val_loss: 0.6921 - val_precision: 0.7718 - val_recall: 0.7153 - val_fbeta_score: 0.7588 - val_accuracy: 0.7570 - val_auc: 0.8816\n",
      "Epoch 2/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.7936 - precision: 0.6966 - recall: 0.5887 - fbeta_score: 0.6641 - accuracy: 0.6685 - auc: 0.8257 - val_loss: 0.6907 - val_precision: 0.6659 - val_recall: 0.6659 - val_fbeta_score: 0.6659 - val_accuracy: 0.6638 - val_auc: 0.8798\n",
      "Epoch 3/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.7336 - precision: 0.6861 - recall: 0.6254 - fbeta_score: 0.6636 - accuracy: 0.6645 - auc: 0.8452 - val_loss: 0.6519 - val_precision: 0.6667 - val_recall: 0.5586 - val_fbeta_score: 0.6403 - val_accuracy: 0.6638 - val_auc: 0.8742\n",
      "Epoch 4/25\n",
      "322/322 [==============================] - 428s 1s/step - loss: 0.7149 - precision: 0.7565 - recall: 0.5380 - fbeta_score: 0.6835 - accuracy: 0.6643 - auc: 0.8503 - val_loss: 0.7631 - val_precision: 0.6667 - val_recall: 0.4136 - val_fbeta_score: 0.5859 - val_accuracy: 0.6731 - val_auc: 0.8487\n",
      "Epoch 5/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6987 - precision: 0.7864 - recall: 0.4965 - fbeta_score: 0.6888 - accuracy: 0.6621 - auc: 0.8554 - val_loss: 0.6279 - val_precision: 0.6667 - val_recall: 0.5949 - val_fbeta_score: 0.6502 - val_accuracy: 0.7772 - val_auc: 0.8990\n",
      "Epoch 6/25\n",
      "322/322 [==============================] - 617s 2s/step - loss: 0.6884 - precision: 0.7968 - recall: 0.5074 - fbeta_score: 0.7009 - accuracy: 0.6641 - auc: 0.8587 - val_loss: 0.8872 - val_precision: 0.6667 - val_recall: 0.1937 - val_fbeta_score: 0.4243 - val_accuracy: 0.6638 - val_auc: 0.8160\n",
      "Epoch 7/25\n",
      "322/322 [==============================] - 801s 2s/step - loss: 0.6870 - precision: 0.7988 - recall: 0.5008 - fbeta_score: 0.6993 - accuracy: 0.6691 - auc: 0.8591 - val_loss: 0.6538 - val_precision: 0.6667 - val_recall: 0.5231 - val_fbeta_score: 0.6294 - val_accuracy: 0.7547 - val_auc: 0.8891\n",
      "Epoch 8/25\n",
      "322/322 [==============================] - 772s 2s/step - loss: 0.6897 - precision: 0.8120 - recall: 0.4804 - fbeta_score: 0.6999 - accuracy: 0.6639 - auc: 0.8581 - val_loss: 0.6191 - val_precision: 0.6667 - val_recall: 0.6034 - val_fbeta_score: 0.6522 - val_accuracy: 0.6638 - val_auc: 0.8875\n",
      "Epoch 9/25\n",
      "322/322 [==============================] - 787s 2s/step - loss: 0.6856 - precision: 0.8111 - recall: 0.4953 - fbeta_score: 0.7050 - accuracy: 0.6664 - auc: 0.8590 - val_loss: 0.6424 - val_precision: 0.6667 - val_recall: 0.5363 - val_fbeta_score: 0.6337 - val_accuracy: 0.6638 - val_auc: 0.8768\n",
      "Epoch 10/25\n",
      "322/322 [==============================] - 724s 2s/step - loss: 0.6807 - precision: 0.7982 - recall: 0.5039 - fbeta_score: 0.6975 - accuracy: 0.6736 - auc: 0.8627 - val_loss: 0.7423 - val_precision: 0.6667 - val_recall: 0.4329 - val_fbeta_score: 0.5933 - val_accuracy: 0.6638 - val_auc: 0.8495\n",
      "Epoch 11/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6859 - precision: 0.8203 - recall: 0.4709 - fbeta_score: 0.6969 - accuracy: 0.6586 - auc: 0.8571 - val_loss: 0.6644 - val_precision: 0.6667 - val_recall: 0.5108 - val_fbeta_score: 0.6251 - val_accuracy: 0.7500 - val_auc: 0.8875\n",
      "Epoch 12/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.6785 - precision: 0.8118 - recall: 0.4885 - fbeta_score: 0.7009 - accuracy: 0.6728 - auc: 0.8615 - val_loss: 0.6054 - val_precision: 0.6667 - val_recall: 0.6134 - val_fbeta_score: 0.6547 - val_accuracy: 0.7997 - val_auc: 0.9067\n",
      "Epoch 13/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6874 - precision: 0.7893 - recall: 0.5151 - fbeta_score: 0.6956 - accuracy: 0.6722 - auc: 0.8600 - val_loss: 0.5897 - val_precision: 0.8106 - val_recall: 0.7955 - val_fbeta_score: 0.8073 - val_accuracy: 0.8036 - val_auc: 0.9094\n",
      "Epoch 14/25\n",
      "322/322 [==============================] - 412s 1s/step - loss: 0.6776 - precision: 0.7605 - recall: 0.5658 - fbeta_score: 0.6967 - accuracy: 0.6787 - auc: 0.8644 - val_loss: 0.6323 - val_precision: 0.7726 - val_recall: 0.7569 - val_fbeta_score: 0.7692 - val_accuracy: 0.7694 - val_auc: 0.8963\n",
      "Epoch 15/25\n",
      "322/322 [==============================] - 2597s 8s/step - loss: 0.6675 - precision: 0.7786 - recall: 0.5569 - fbeta_score: 0.7060 - accuracy: 0.6855 - auc: 0.8678 - val_loss: 0.6085 - val_precision: 0.6667 - val_recall: 0.5648 - val_fbeta_score: 0.6423 - val_accuracy: 0.7943 - val_auc: 0.9068\n",
      "Epoch 16/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6735 - precision: 0.8386 - recall: 0.4618 - fbeta_score: 0.7083 - accuracy: 0.6680 - auc: 0.8629 - val_loss: 0.6081 - val_precision: 0.6667 - val_recall: 0.5934 - val_fbeta_score: 0.6497 - val_accuracy: 0.8043 - val_auc: 0.9055\n",
      "Epoch 17/25\n",
      "322/322 [==============================] - 608s 2s/step - loss: 0.6778 - precision: 0.8187 - recall: 0.4847 - fbeta_score: 0.7039 - accuracy: 0.6664 - auc: 0.8611 - val_loss: 0.6222 - val_precision: 0.6667 - val_recall: 0.5610 - val_fbeta_score: 0.6413 - val_accuracy: 0.7888 - val_auc: 0.9023\n",
      "Epoch 18/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6717 - precision: 0.8266 - recall: 0.4779 - fbeta_score: 0.7093 - accuracy: 0.6814 - auc: 0.8661 - val_loss: 0.5939 - val_precision: 0.6667 - val_recall: 0.5810 - val_fbeta_score: 0.6464 - val_accuracy: 0.7997 - val_auc: 0.9114\n",
      "Epoch 19/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6641 - precision: 0.7523 - recall: 0.6091 - fbeta_score: 0.7064 - accuracy: 0.6938 - auc: 0.8731 - val_loss: 0.6648 - val_precision: 0.7488 - val_recall: 0.7276 - val_fbeta_score: 0.7442 - val_accuracy: 0.7484 - val_auc: 0.8867\n",
      "Epoch 20/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6695 - precision: 0.7392 - recall: 0.6335 - fbeta_score: 0.7061 - accuracy: 0.6923 - auc: 0.8713 - val_loss: 0.6810 - val_precision: 0.7344 - val_recall: 0.7160 - val_fbeta_score: 0.7305 - val_accuracy: 0.7314 - val_auc: 0.8779\n",
      "Epoch 21/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6809 - precision: 0.7955 - recall: 0.5163 - fbeta_score: 0.7034 - accuracy: 0.6790 - auc: 0.8646 - val_loss: 0.5985 - val_precision: 0.8064 - val_recall: 0.7917 - val_fbeta_score: 0.8033 - val_accuracy: 0.8005 - val_auc: 0.9116\n",
      "Epoch 22/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6690 - precision: 0.8060 - recall: 0.5252 - fbeta_score: 0.7121 - accuracy: 0.6897 - auc: 0.8705 - val_loss: 0.6100 - val_precision: 0.8030 - val_recall: 0.7847 - val_fbeta_score: 0.7991 - val_accuracy: 0.7958 - val_auc: 0.9075\n",
      "Epoch 23/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.6574 - precision: 0.7834 - recall: 0.5598 - fbeta_score: 0.7084 - accuracy: 0.6948 - auc: 0.8732 - val_loss: 0.6822 - val_precision: 0.7362 - val_recall: 0.7191 - val_fbeta_score: 0.7325 - val_accuracy: 0.7314 - val_auc: 0.8800\n",
      "Epoch 24/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6578 - precision: 0.7729 - recall: 0.5629 - fbeta_score: 0.7036 - accuracy: 0.6903 - auc: 0.8727 - val_loss: 0.6017 - val_precision: 0.8029 - val_recall: 0.7847 - val_fbeta_score: 0.7990 - val_accuracy: 0.8005 - val_auc: 0.9099\n",
      "Epoch 25/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.6665 - precision: 0.7654 - recall: 0.5728 - fbeta_score: 0.7033 - accuracy: 0.6829 - auc: 0.8695 - val_loss: 0.5776 - val_precision: 0.6667 - val_recall: 0.6142 - val_fbeta_score: 0.6549 - val_accuracy: 0.8199 - val_auc: 0.9168\n",
      "training time: 240.33 minutes\n",
      "81/81 [==============================] - 73s 903ms/step - loss: 0.5776 - precision: 0.6667 - recall: 0.6142 - fbeta_score: 0.6549 - accuracy: 0.8199 - auc: 0.9168\n",
      "precision: 66.67%\n",
      "Found 5144 images belonging to 3 classes.\n",
      "Found 1288 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.6644 - precision: 0.7944 - recall: 0.5431 - fbeta_score: 0.7115 - accuracy: 0.6903 - auc: 0.8710 - val_loss: 0.5870 - val_precision: 0.8094 - val_recall: 0.7940 - val_fbeta_score: 0.8060 - val_accuracy: 0.8051 - val_auc: 0.9161\n",
      "Epoch 2/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.6598 - precision: 0.7659 - recall: 0.5982 - fbeta_score: 0.7117 - accuracy: 0.6948 - auc: 0.8739 - val_loss: 0.5983 - val_precision: 0.6667 - val_recall: 0.5733 - val_fbeta_score: 0.6446 - val_accuracy: 0.8059 - val_auc: 0.9160\n",
      "Epoch 3/25\n",
      "322/322 [==============================] - 424s 1s/step - loss: 0.6585 - precision: 0.7878 - recall: 0.5474 - fbeta_score: 0.7089 - accuracy: 0.6915 - auc: 0.8731 - val_loss: 0.5835 - val_precision: 0.6667 - val_recall: 0.5872 - val_fbeta_score: 0.6481 - val_accuracy: 0.8067 - val_auc: 0.9175\n",
      "Epoch 4/25\n",
      "322/322 [==============================] - 428s 1s/step - loss: 0.6549 - precision: 0.7951 - recall: 0.5606 - fbeta_score: 0.7175 - accuracy: 0.6928 - auc: 0.8762 - val_loss: 0.6696 - val_precision: 0.7569 - val_recall: 0.7353 - val_fbeta_score: 0.7522 - val_accuracy: 0.7547 - val_auc: 0.8895\n",
      "Epoch 5/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.6599 - precision: 0.7836 - recall: 0.5553 - fbeta_score: 0.7096 - accuracy: 0.6893 - auc: 0.8732 - val_loss: 0.6314 - val_precision: 0.6667 - val_recall: 0.5417 - val_fbeta_score: 0.6350 - val_accuracy: 0.7873 - val_auc: 0.9045\n",
      "Epoch 6/25\n",
      "322/322 [==============================] - 426s 1s/step - loss: 0.6569 - precision: 0.7684 - recall: 0.5850 - fbeta_score: 0.7085 - accuracy: 0.6897 - auc: 0.8741 - val_loss: 0.5918 - val_precision: 0.6667 - val_recall: 0.5864 - val_fbeta_score: 0.6478 - val_accuracy: 0.8113 - val_auc: 0.9156\n",
      "Epoch 7/25\n",
      "322/322 [==============================] - 411s 1s/step - loss: 0.6627 - precision: 0.8177 - recall: 0.5175 - fbeta_score: 0.7144 - accuracy: 0.6755 - auc: 0.8710 - val_loss: 0.5882 - val_precision: 0.8123 - val_recall: 0.7878 - val_fbeta_score: 0.8070 - val_accuracy: 0.8082 - val_auc: 0.9161\n",
      "Epoch 8/25\n",
      "322/322 [==============================] - 410s 1s/step - loss: 0.6520 - precision: 0.8004 - recall: 0.5604 - fbeta_score: 0.7203 - accuracy: 0.6956 - auc: 0.8771 - val_loss: 0.5792 - val_precision: 0.8147 - val_recall: 0.7894 - val_fbeta_score: 0.8092 - val_accuracy: 0.8082 - val_auc: 0.9182\n",
      "Epoch 9/25\n",
      "322/322 [==============================] - 410s 1s/step - loss: 0.6541 - precision: 0.7492 - recall: 0.6186 - fbeta_score: 0.7076 - accuracy: 0.6930 - auc: 0.8770 - val_loss: 0.5746 - val_precision: 0.6667 - val_recall: 0.6235 - val_fbeta_score: 0.6571 - val_accuracy: 0.6638 - val_auc: 0.9104\n",
      "Epoch 10/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6581 - precision: 0.7758 - recall: 0.5817 - fbeta_score: 0.7124 - accuracy: 0.6956 - auc: 0.8760 - val_loss: 0.5693 - val_precision: 0.6667 - val_recall: 0.5995 - val_fbeta_score: 0.6513 - val_accuracy: 0.8230 - val_auc: 0.9221\n",
      "Epoch 11/25\n",
      "322/322 [==============================] - 412s 1s/step - loss: 0.6515 - precision: 0.7838 - recall: 0.5780 - fbeta_score: 0.7150 - accuracy: 0.6962 - auc: 0.8779 - val_loss: 0.5632 - val_precision: 0.8251 - val_recall: 0.7971 - val_fbeta_score: 0.8191 - val_accuracy: 0.8144 - val_auc: 0.9224\n",
      "Epoch 12/25\n",
      "322/322 [==============================] - 408s 1s/step - loss: 0.6515 - precision: 0.7867 - recall: 0.5864 - fbeta_score: 0.7208 - accuracy: 0.6989 - auc: 0.8791 - val_loss: 0.5690 - val_precision: 0.6667 - val_recall: 0.6327 - val_fbeta_score: 0.6593 - val_accuracy: 0.8144 - val_auc: 0.9198\n",
      "Epoch 13/25\n",
      "322/322 [==============================] - 411s 1s/step - loss: 0.6622 - precision: 0.8093 - recall: 0.5250 - fbeta_score: 0.7153 - accuracy: 0.6923 - auc: 0.8733 - val_loss: 0.5764 - val_precision: 0.6667 - val_recall: 0.6381 - val_fbeta_score: 0.6605 - val_accuracy: 0.8106 - val_auc: 0.9184\n",
      "Epoch 14/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6455 - precision: 0.7476 - recall: 0.6374 - fbeta_score: 0.7140 - accuracy: 0.7063 - auc: 0.8806 - val_loss: 0.5899 - val_precision: 0.6667 - val_recall: 0.6427 - val_fbeta_score: 0.6615 - val_accuracy: 0.7974 - val_auc: 0.9142\n",
      "Epoch 15/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.6465 - precision: 0.7882 - recall: 0.5664 - fbeta_score: 0.7127 - accuracy: 0.6950 - auc: 0.8794 - val_loss: 0.5924 - val_precision: 0.6667 - val_recall: 0.5733 - val_fbeta_score: 0.6444 - val_accuracy: 0.8090 - val_auc: 0.9168\n",
      "Epoch 16/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6436 - precision: 0.7699 - recall: 0.6192 - fbeta_score: 0.7220 - accuracy: 0.7072 - auc: 0.8824 - val_loss: 0.5713 - val_precision: 0.8268 - val_recall: 0.7948 - val_fbeta_score: 0.8199 - val_accuracy: 0.8137 - val_auc: 0.9230\n",
      "Epoch 17/25\n",
      "322/322 [==============================] - 1395s 4s/step - loss: 0.6461 - precision: 0.7645 - recall: 0.6033 - fbeta_score: 0.7124 - accuracy: 0.7004 - auc: 0.8802 - val_loss: 0.7504 - val_precision: 0.7087 - val_recall: 0.6613 - val_fbeta_score: 0.6978 - val_accuracy: 0.7065 - val_auc: 0.8599\n",
      "Epoch 18/25\n",
      "322/322 [==============================] - 409s 1s/step - loss: 0.6513 - precision: 0.7759 - recall: 0.5901 - fbeta_score: 0.7169 - accuracy: 0.6940 - auc: 0.8787 - val_loss: 0.6506 - val_precision: 0.6667 - val_recall: 0.5170 - val_fbeta_score: 0.6270 - val_accuracy: 0.7725 - val_auc: 0.9019\n",
      "Epoch 19/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6494 - precision: 0.7863 - recall: 0.5652 - fbeta_score: 0.7129 - accuracy: 0.6899 - auc: 0.8776 - val_loss: 0.6646 - val_precision: 0.6667 - val_recall: 0.5054 - val_fbeta_score: 0.6229 - val_accuracy: 0.7624 - val_auc: 0.8934\n",
      "Epoch 20/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.6572 - precision: 0.8371 - recall: 0.5064 - fbeta_score: 0.7234 - accuracy: 0.6950 - auc: 0.8761 - val_loss: 0.5554 - val_precision: 0.6667 - val_recall: 0.6119 - val_fbeta_score: 0.6544 - val_accuracy: 0.8238 - val_auc: 0.9237\n",
      "Epoch 21/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.6505 - precision: 0.8158 - recall: 0.5221 - fbeta_score: 0.7126 - accuracy: 0.6860 - auc: 0.8767 - val_loss: 0.5598 - val_precision: 0.6667 - val_recall: 0.6127 - val_fbeta_score: 0.6546 - val_accuracy: 0.8245 - val_auc: 0.9240\n",
      "Epoch 22/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.6526 - precision: 0.8096 - recall: 0.5437 - fbeta_score: 0.7215 - accuracy: 0.6975 - auc: 0.8778 - val_loss: 0.5712 - val_precision: 0.6667 - val_recall: 0.5903 - val_fbeta_score: 0.6488 - val_accuracy: 0.8214 - val_auc: 0.9241\n",
      "Epoch 23/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6436 - precision: 0.8178 - recall: 0.5421 - fbeta_score: 0.7241 - accuracy: 0.6985 - auc: 0.8807 - val_loss: 0.5615 - val_precision: 0.6667 - val_recall: 0.6003 - val_fbeta_score: 0.6514 - val_accuracy: 0.8269 - val_auc: 0.9249\n",
      "Epoch 24/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6516 - precision: 0.8170 - recall: 0.5229 - fbeta_score: 0.7182 - accuracy: 0.6874 - auc: 0.8765 - val_loss: 0.5546 - val_precision: 0.6667 - val_recall: 0.5934 - val_fbeta_score: 0.6496 - val_accuracy: 0.8238 - val_auc: 0.9256\n",
      "Epoch 25/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6484 - precision: 0.8094 - recall: 0.5501 - fbeta_score: 0.7172 - accuracy: 0.6958 - auc: 0.8788 - val_loss: 0.6609 - val_precision: 0.6667 - val_recall: 0.5093 - val_fbeta_score: 0.6241 - val_accuracy: 0.7679 - val_auc: 0.8963\n",
      "training time: 190.02 minutes\n",
      "81/81 [==============================] - 73s 897ms/step - loss: 0.6609 - precision: 0.6667 - recall: 0.5093 - fbeta_score: 0.6241 - accuracy: 0.7679 - auc: 0.8963\n",
      "precision: 66.67%\n",
      "Found 5144 images belonging to 3 classes.\n",
      "Found 1288 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6534 - precision: 0.8213 - recall: 0.5330 - fbeta_score: 0.7252 - accuracy: 0.6942 - auc: 0.8777 - val_loss: 0.5755 - val_precision: 0.8389 - val_recall: 0.7886 - val_fbeta_score: 0.8277 - val_accuracy: 0.8214 - val_auc: 0.9229\n",
      "Epoch 2/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.6425 - precision: 0.7916 - recall: 0.5648 - fbeta_score: 0.7169 - accuracy: 0.6993 - auc: 0.8806 - val_loss: 0.6214 - val_precision: 0.6667 - val_recall: 0.5324 - val_fbeta_score: 0.6322 - val_accuracy: 0.7911 - val_auc: 0.9112\n",
      "Epoch 3/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6460 - precision: 0.8297 - recall: 0.5190 - fbeta_score: 0.7247 - accuracy: 0.6942 - auc: 0.8797 - val_loss: 0.5779 - val_precision: 0.8221 - val_recall: 0.7716 - val_fbeta_score: 0.8109 - val_accuracy: 0.8121 - val_auc: 0.9202\n",
      "Epoch 4/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6449 - precision: 0.7717 - recall: 0.6005 - fbeta_score: 0.7174 - accuracy: 0.7035 - auc: 0.8818 - val_loss: 0.5463 - val_precision: 0.8383 - val_recall: 0.8025 - val_fbeta_score: 0.8304 - val_accuracy: 0.8276 - val_auc: 0.9270\n",
      "Epoch 5/25\n",
      "322/322 [==============================] - 611s 2s/step - loss: 0.6369 - precision: 0.7658 - recall: 0.6275 - fbeta_score: 0.7223 - accuracy: 0.7024 - auc: 0.8836 - val_loss: 0.5845 - val_precision: 0.8323 - val_recall: 0.7708 - val_fbeta_score: 0.8185 - val_accuracy: 0.8106 - val_auc: 0.9211\n",
      "Epoch 6/25\n",
      "322/322 [==============================] - 757s 2s/step - loss: 0.6517 - precision: 0.8053 - recall: 0.5297 - fbeta_score: 0.7106 - accuracy: 0.6847 - auc: 0.8764 - val_loss: 0.6247 - val_precision: 0.6667 - val_recall: 0.5255 - val_fbeta_score: 0.6300 - val_accuracy: 0.7857 - val_auc: 0.9084\n",
      "Epoch 7/25\n",
      "322/322 [==============================] - 756s 2s/step - loss: 0.6501 - precision: 0.8379 - recall: 0.5047 - fbeta_score: 0.7268 - accuracy: 0.6934 - auc: 0.8778 - val_loss: 0.5878 - val_precision: 0.6667 - val_recall: 0.5556 - val_fbeta_score: 0.6394 - val_accuracy: 0.8067 - val_auc: 0.9207\n",
      "Epoch 8/25\n",
      "322/322 [==============================] - 755s 2s/step - loss: 0.6430 - precision: 0.8443 - recall: 0.4961 - fbeta_score: 0.7255 - accuracy: 0.6962 - auc: 0.8795 - val_loss: 0.5716 - val_precision: 0.6667 - val_recall: 0.5625 - val_fbeta_score: 0.6413 - val_accuracy: 0.8137 - val_auc: 0.9235\n",
      "Epoch 9/25\n",
      "322/322 [==============================] - 752s 2s/step - loss: 0.6627 - precision: 0.7039 - recall: 0.6165 - fbeta_score: 0.6671 - accuracy: 0.6810 - auc: 0.8723 - val_loss: 0.6046 - val_precision: 0.6658 - val_recall: 0.6227 - val_fbeta_score: 0.6562 - val_accuracy: 0.6638 - val_auc: 0.8945\n",
      "Epoch 10/25\n",
      "322/322 [==============================] - 23160s 72s/step - loss: 0.6413 - precision: 0.8582 - recall: 0.4800 - fbeta_score: 0.7276 - accuracy: 0.6876 - auc: 0.8797 - val_loss: 0.5607 - val_precision: 0.6667 - val_recall: 0.6273 - val_fbeta_score: 0.6580 - val_accuracy: 0.8214 - val_auc: 0.9241\n",
      "Epoch 11/25\n",
      "322/322 [==============================] - 481s 1s/step - loss: 0.6420 - precision: 0.8322 - recall: 0.5283 - fbeta_score: 0.7305 - accuracy: 0.6979 - auc: 0.8811 - val_loss: 0.5485 - val_precision: 0.6667 - val_recall: 0.6080 - val_fbeta_score: 0.6533 - val_accuracy: 0.8261 - val_auc: 0.9269\n",
      "Epoch 12/25\n",
      "322/322 [==============================] - 421s 1s/step - loss: 0.6421 - precision: 0.8896 - recall: 0.4618 - fbeta_score: 0.7374 - accuracy: 0.6917 - auc: 0.8780 - val_loss: 0.5688 - val_precision: 0.6667 - val_recall: 0.5579 - val_fbeta_score: 0.6401 - val_accuracy: 0.8020 - val_auc: 0.9216\n",
      "Epoch 13/25\n",
      "322/322 [==============================] - 420s 1s/step - loss: 0.6354 - precision: 0.8390 - recall: 0.5202 - fbeta_score: 0.7284 - accuracy: 0.6952 - auc: 0.8821 - val_loss: 0.5566 - val_precision: 0.6667 - val_recall: 0.5864 - val_fbeta_score: 0.6478 - val_accuracy: 0.8230 - val_auc: 0.9262\n",
      "Epoch 14/25\n",
      "322/322 [==============================] - 421s 1s/step - loss: 0.6450 - precision: 0.8606 - recall: 0.4926 - fbeta_score: 0.7311 - accuracy: 0.6934 - auc: 0.8801 - val_loss: 0.6007 - val_precision: 0.6667 - val_recall: 0.5401 - val_fbeta_score: 0.6346 - val_accuracy: 0.7943 - val_auc: 0.9159\n",
      "Epoch 15/25\n",
      "322/322 [==============================] - 430s 1s/step - loss: 0.6325 - precision: 0.8372 - recall: 0.5297 - fbeta_score: 0.7313 - accuracy: 0.7012 - auc: 0.8844 - val_loss: 0.5784 - val_precision: 0.8292 - val_recall: 0.7708 - val_fbeta_score: 0.8160 - val_accuracy: 0.8144 - val_auc: 0.9216\n",
      "Epoch 16/25\n",
      "322/322 [==============================] - 410s 1s/step - loss: 0.6328 - precision: 0.7930 - recall: 0.5714 - fbeta_score: 0.7202 - accuracy: 0.6983 - auc: 0.8839 - val_loss: 0.5978 - val_precision: 0.8194 - val_recall: 0.7554 - val_fbeta_score: 0.8046 - val_accuracy: 0.7989 - val_auc: 0.9150\n",
      "Epoch 17/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6462 - precision: 0.8865 - recall: 0.4653 - fbeta_score: 0.7370 - accuracy: 0.6923 - auc: 0.8787 - val_loss: 0.5643 - val_precision: 0.6667 - val_recall: 0.5625 - val_fbeta_score: 0.6413 - val_accuracy: 0.8137 - val_auc: 0.9245\n",
      "Epoch 18/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6302 - precision: 0.8112 - recall: 0.5722 - fbeta_score: 0.7318 - accuracy: 0.7030 - auc: 0.8858 - val_loss: 0.5621 - val_precision: 0.6667 - val_recall: 0.5679 - val_fbeta_score: 0.6427 - val_accuracy: 0.8191 - val_auc: 0.9262\n",
      "Epoch 19/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6363 - precision: 0.8139 - recall: 0.5648 - fbeta_score: 0.7308 - accuracy: 0.7035 - auc: 0.8847 - val_loss: 0.6021 - val_precision: 0.8184 - val_recall: 0.7593 - val_fbeta_score: 0.8048 - val_accuracy: 0.7950 - val_auc: 0.9144\n",
      "Epoch 20/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6235 - precision: 0.7906 - recall: 0.5887 - fbeta_score: 0.7254 - accuracy: 0.7018 - auc: 0.8869 - val_loss: 0.5917 - val_precision: 0.6667 - val_recall: 0.5417 - val_fbeta_score: 0.6349 - val_accuracy: 0.8012 - val_auc: 0.9186\n",
      "Epoch 21/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6381 - precision: 0.8406 - recall: 0.5344 - fbeta_score: 0.7359 - accuracy: 0.7000 - auc: 0.8833 - val_loss: 0.6125 - val_precision: 0.8217 - val_recall: 0.7485 - val_fbeta_score: 0.8045 - val_accuracy: 0.7981 - val_auc: 0.9142\n",
      "Epoch 22/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6288 - precision: 0.8089 - recall: 0.5677 - fbeta_score: 0.7285 - accuracy: 0.7016 - auc: 0.8853 - val_loss: 0.5569 - val_precision: 0.6667 - val_recall: 0.5802 - val_fbeta_score: 0.6461 - val_accuracy: 0.8269 - val_auc: 0.9289\n",
      "Epoch 23/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.6293 - precision: 0.8649 - recall: 0.4988 - fbeta_score: 0.7403 - accuracy: 0.6981 - auc: 0.8841 - val_loss: 0.5703 - val_precision: 0.6667 - val_recall: 0.5594 - val_fbeta_score: 0.6405 - val_accuracy: 0.8067 - val_auc: 0.9224\n",
      "Epoch 24/25\n",
      "322/322 [==============================] - 411s 1s/step - loss: 0.6319 - precision: 0.8051 - recall: 0.5666 - fbeta_score: 0.7221 - accuracy: 0.7006 - auc: 0.8845 - val_loss: 0.5409 - val_precision: 0.6667 - val_recall: 0.6165 - val_fbeta_score: 0.6555 - val_accuracy: 0.8238 - val_auc: 0.9285\n",
      "Epoch 25/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.6257 - precision: 0.8151 - recall: 0.5652 - fbeta_score: 0.7348 - accuracy: 0.7076 - auc: 0.8877 - val_loss: 0.5400 - val_precision: 0.6667 - val_recall: 0.6003 - val_fbeta_score: 0.6514 - val_accuracy: 0.8300 - val_auc: 0.9308\n",
      "training time: 580.33 minutes\n",
      "81/81 [==============================] - 76s 935ms/step - loss: 0.5400 - precision: 0.6667 - recall: 0.6003 - fbeta_score: 0.6514 - accuracy: 0.8300 - auc: 0.9308\n",
      "precision: 66.67%\n",
      "Found 5144 images belonging to 3 classes.\n",
      "Found 1288 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6372 - precision: 0.8744 - recall: 0.4839 - fbeta_score: 0.7383 - accuracy: 0.6925 - auc: 0.8811 - val_loss: 0.5953 - val_precision: 0.6667 - val_recall: 0.5455 - val_fbeta_score: 0.6361 - val_accuracy: 0.8067 - val_auc: 0.9188\n",
      "Epoch 2/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.6299 - precision: 0.8141 - recall: 0.5551 - fbeta_score: 0.7279 - accuracy: 0.7016 - auc: 0.8859 - val_loss: 0.5483 - val_precision: 0.6667 - val_recall: 0.5980 - val_fbeta_score: 0.6507 - val_accuracy: 0.8253 - val_auc: 0.9293\n",
      "Epoch 3/25\n",
      "322/322 [==============================] - 414s 1s/step - loss: 0.6434 - precision: 0.8322 - recall: 0.5153 - fbeta_score: 0.7243 - accuracy: 0.6903 - auc: 0.8797 - val_loss: 0.5542 - val_precision: 0.6667 - val_recall: 0.5764 - val_fbeta_score: 0.6451 - val_accuracy: 0.8253 - val_auc: 0.9275\n",
      "Epoch 4/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6234 - precision: 0.7951 - recall: 0.5794 - fbeta_score: 0.7224 - accuracy: 0.7047 - auc: 0.8879 - val_loss: 0.5374 - val_precision: 0.6667 - val_recall: 0.5864 - val_fbeta_score: 0.6477 - val_accuracy: 0.8253 - val_auc: 0.9299\n",
      "Epoch 5/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6270 - precision: 0.7978 - recall: 0.5792 - fbeta_score: 0.7253 - accuracy: 0.7030 - auc: 0.8862 - val_loss: 0.6778 - val_precision: 0.6667 - val_recall: 0.4807 - val_fbeta_score: 0.6132 - val_accuracy: 0.7640 - val_auc: 0.8953\n",
      "Epoch 6/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6328 - precision: 0.8834 - recall: 0.4812 - fbeta_score: 0.7449 - accuracy: 0.6973 - auc: 0.8845 - val_loss: 0.5642 - val_precision: 0.6667 - val_recall: 0.5671 - val_fbeta_score: 0.6425 - val_accuracy: 0.8222 - val_auc: 0.9280\n",
      "Epoch 7/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6387 - precision: 0.8806 - recall: 0.4763 - fbeta_score: 0.7393 - accuracy: 0.6995 - auc: 0.8827 - val_loss: 0.5823 - val_precision: 0.8344 - val_recall: 0.7569 - val_fbeta_score: 0.8165 - val_accuracy: 0.7927 - val_auc: 0.9184\n",
      "Epoch 8/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.6252 - precision: 0.7818 - recall: 0.5984 - fbeta_score: 0.7212 - accuracy: 0.7066 - auc: 0.8872 - val_loss: 0.5840 - val_precision: 0.8344 - val_recall: 0.7523 - val_fbeta_score: 0.8155 - val_accuracy: 0.7974 - val_auc: 0.9201\n",
      "Epoch 9/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6291 - precision: 0.8279 - recall: 0.5512 - fbeta_score: 0.7337 - accuracy: 0.7031 - auc: 0.8860 - val_loss: 0.6761 - val_precision: 0.7998 - val_recall: 0.7137 - val_fbeta_score: 0.7791 - val_accuracy: 0.7686 - val_auc: 0.8961\n",
      "Epoch 10/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6278 - precision: 0.7970 - recall: 0.5813 - fbeta_score: 0.7275 - accuracy: 0.7033 - auc: 0.8868 - val_loss: 0.5299 - val_precision: 0.6667 - val_recall: 0.6103 - val_fbeta_score: 0.6539 - val_accuracy: 0.8276 - val_auc: 0.9305\n",
      "Epoch 11/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6247 - precision: 0.8150 - recall: 0.5615 - fbeta_score: 0.7324 - accuracy: 0.7041 - auc: 0.8884 - val_loss: 0.5752 - val_precision: 0.8425 - val_recall: 0.7554 - val_fbeta_score: 0.8224 - val_accuracy: 0.8067 - val_auc: 0.9221\n",
      "Epoch 12/25\n",
      "322/322 [==============================] - 515s 2s/step - loss: 0.6248 - precision: 0.7751 - recall: 0.6097 - fbeta_score: 0.7249 - accuracy: 0.7090 - auc: 0.8887 - val_loss: 0.5895 - val_precision: 0.8357 - val_recall: 0.7492 - val_fbeta_score: 0.8156 - val_accuracy: 0.7958 - val_auc: 0.9192\n",
      "Epoch 13/25\n",
      "322/322 [==============================] - 760s 2s/step - loss: 0.6323 - precision: 0.8601 - recall: 0.5113 - fbeta_score: 0.7410 - accuracy: 0.6973 - auc: 0.8845 - val_loss: 0.5371 - val_precision: 0.6667 - val_recall: 0.5841 - val_fbeta_score: 0.6472 - val_accuracy: 0.8269 - val_auc: 0.9308\n",
      "Epoch 14/25\n",
      "322/322 [==============================] - 516s 2s/step - loss: 0.6315 - precision: 0.8413 - recall: 0.5322 - fbeta_score: 0.7378 - accuracy: 0.6958 - auc: 0.8855 - val_loss: 0.5675 - val_precision: 0.6667 - val_recall: 0.5571 - val_fbeta_score: 0.6398 - val_accuracy: 0.8098 - val_auc: 0.9247\n",
      "Epoch 15/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6305 - precision: 0.8687 - recall: 0.4874 - fbeta_score: 0.7373 - accuracy: 0.6958 - auc: 0.8841 - val_loss: 0.5742 - val_precision: 0.8437 - val_recall: 0.7508 - val_fbeta_score: 0.8221 - val_accuracy: 0.7997 - val_auc: 0.9234\n",
      "Epoch 16/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.6244 - precision: 0.8352 - recall: 0.5318 - fbeta_score: 0.7337 - accuracy: 0.7030 - auc: 0.8882 - val_loss: 0.5685 - val_precision: 0.8467 - val_recall: 0.7438 - val_fbeta_score: 0.8226 - val_accuracy: 0.8121 - val_auc: 0.9264\n",
      "Epoch 17/25\n",
      "322/322 [==============================] - 427s 1s/step - loss: 0.6206 - precision: 0.7903 - recall: 0.5998 - fbeta_score: 0.7313 - accuracy: 0.7090 - auc: 0.8901 - val_loss: 0.5465 - val_precision: 0.8143 - val_recall: 0.5926 - val_fbeta_score: 0.6986 - val_accuracy: 0.8199 - val_auc: 0.9277\n",
      "Epoch 18/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.6228 - precision: 0.8210 - recall: 0.5557 - fbeta_score: 0.7381 - accuracy: 0.7024 - auc: 0.8904 - val_loss: 0.5455 - val_precision: 0.8348 - val_recall: 0.6181 - val_fbeta_score: 0.7297 - val_accuracy: 0.8261 - val_auc: 0.9285\n",
      "Epoch 19/25\n",
      "322/322 [==============================] - 418s 1s/step - loss: 0.6200 - precision: 0.8663 - recall: 0.5314 - fbeta_score: 0.7623 - accuracy: 0.7076 - auc: 0.8925 - val_loss: 0.6613 - val_precision: 0.8134 - val_recall: 0.5409 - val_fbeta_score: 0.6801 - val_accuracy: 0.7780 - val_auc: 0.8980\n",
      "Epoch 20/25\n",
      "322/322 [==============================] - 421s 1s/step - loss: 0.6346 - precision: 0.8368 - recall: 0.5639 - fbeta_score: 0.7562 - accuracy: 0.7146 - auc: 0.8893 - val_loss: 0.6563 - val_precision: 0.7645 - val_recall: 0.6728 - val_fbeta_score: 0.7428 - val_accuracy: 0.7376 - val_auc: 0.9086\n",
      "Epoch 21/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.6026 - precision: 0.7878 - recall: 0.7275 - fbeta_score: 0.7743 - accuracy: 0.7615 - auc: 0.9032 - val_loss: 0.5023 - val_precision: 0.8290 - val_recall: 0.7894 - val_fbeta_score: 0.8201 - val_accuracy: 0.8207 - val_auc: 0.9333\n",
      "Epoch 22/25\n",
      "322/322 [==============================] - 921s 3s/step - loss: 0.5862 - precision: 0.7951 - recall: 0.7471 - fbeta_score: 0.7845 - accuracy: 0.7741 - auc: 0.9084 - val_loss: 0.5651 - val_precision: 0.8366 - val_recall: 0.7639 - val_fbeta_score: 0.8199 - val_accuracy: 0.8121 - val_auc: 0.9333\n",
      "Epoch 23/25\n",
      "322/322 [==============================] - 409s 1s/step - loss: 0.5786 - precision: 0.8005 - recall: 0.7541 - fbeta_score: 0.7902 - accuracy: 0.7792 - auc: 0.9106 - val_loss: 0.5730 - val_precision: 0.8117 - val_recall: 0.7508 - val_fbeta_score: 0.7978 - val_accuracy: 0.7873 - val_auc: 0.9246\n",
      "Epoch 24/25\n",
      "322/322 [==============================] - 412s 1s/step - loss: 0.5716 - precision: 0.7992 - recall: 0.7502 - fbeta_score: 0.7883 - accuracy: 0.7790 - auc: 0.9123 - val_loss: 0.5999 - val_precision: 0.8028 - val_recall: 0.7415 - val_fbeta_score: 0.7886 - val_accuracy: 0.7787 - val_auc: 0.9105\n",
      "Epoch 25/25\n",
      "322/322 [==============================] - 412s 1s/step - loss: 0.5780 - precision: 0.8009 - recall: 0.7517 - fbeta_score: 0.7900 - accuracy: 0.7766 - auc: 0.9104 - val_loss: 0.8900 - val_precision: 0.6580 - val_recall: 0.5054 - val_fbeta_score: 0.6145 - val_accuracy: 0.6382 - val_auc: 0.7728\n",
      "training time: 191.41 minutes\n",
      "81/81 [==============================] - 73s 907ms/step - loss: 0.8900 - precision: 0.6580 - recall: 0.5054 - fbeta_score: 0.6145 - accuracy: 0.6382 - auc: 0.7728\n",
      "precision: 65.80%\n",
      "Found 5144 images belonging to 3 classes.\n",
      "Found 1288 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "322/322 [==============================] - 412s 1s/step - loss: 0.5687 - precision: 0.7975 - recall: 0.7510 - fbeta_score: 0.7873 - accuracy: 0.7759 - auc: 0.9136 - val_loss: 0.6577 - val_precision: 0.7611 - val_recall: 0.6867 - val_fbeta_score: 0.7435 - val_accuracy: 0.7376 - val_auc: 0.8887\n",
      "Epoch 2/25\n",
      "322/322 [==============================] - 411s 1s/step - loss: 0.5959 - precision: 0.7834 - recall: 0.7290 - fbeta_score: 0.7713 - accuracy: 0.7587 - auc: 0.9048 - val_loss: 0.5226 - val_precision: 0.8281 - val_recall: 0.7554 - val_fbeta_score: 0.8114 - val_accuracy: 0.8012 - val_auc: 0.9341\n",
      "Epoch 3/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.5789 - precision: 0.7996 - recall: 0.7432 - fbeta_score: 0.7869 - accuracy: 0.7733 - auc: 0.9114 - val_loss: 0.6839 - val_precision: 0.7929 - val_recall: 0.6759 - val_fbeta_score: 0.7635 - val_accuracy: 0.7593 - val_auc: 0.8792\n",
      "Epoch 4/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.5644 - precision: 0.8057 - recall: 0.7578 - fbeta_score: 0.7951 - accuracy: 0.7830 - auc: 0.9154 - val_loss: 0.4713 - val_precision: 0.8427 - val_recall: 0.8063 - val_fbeta_score: 0.8346 - val_accuracy: 0.8315 - val_auc: 0.9426\n",
      "Epoch 5/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.5574 - precision: 0.8121 - recall: 0.7640 - fbeta_score: 0.8015 - accuracy: 0.7862 - auc: 0.9167 - val_loss: 0.6574 - val_precision: 0.7554 - val_recall: 0.6968 - val_fbeta_score: 0.7417 - val_accuracy: 0.7407 - val_auc: 0.8890\n",
      "Epoch 6/25\n",
      "322/322 [==============================] - 425s 1s/step - loss: 0.5553 - precision: 0.8105 - recall: 0.7642 - fbeta_score: 0.8003 - accuracy: 0.7904 - auc: 0.9178 - val_loss: 0.7779 - val_precision: 0.7160 - val_recall: 0.6088 - val_fbeta_score: 0.6887 - val_accuracy: 0.6918 - val_auc: 0.8319\n",
      "Epoch 7/25\n",
      "322/322 [==============================] - 755s 2s/step - loss: 0.5537 - precision: 0.8176 - recall: 0.7616 - fbeta_score: 0.8051 - accuracy: 0.7928 - auc: 0.9178 - val_loss: 0.5475 - val_precision: 0.8130 - val_recall: 0.7600 - val_fbeta_score: 0.8009 - val_accuracy: 0.7943 - val_auc: 0.9264\n",
      "Epoch 8/25\n",
      "322/322 [==============================] - 759s 2s/step - loss: 0.5583 - precision: 0.8083 - recall: 0.7554 - fbeta_score: 0.7966 - accuracy: 0.7846 - auc: 0.9166 - val_loss: 0.5358 - val_precision: 0.8134 - val_recall: 0.7762 - val_fbeta_score: 0.8051 - val_accuracy: 0.7974 - val_auc: 0.9296\n",
      "Epoch 9/25\n",
      "322/322 [==============================] - 708s 2s/step - loss: 0.5624 - precision: 0.8080 - recall: 0.7624 - fbeta_score: 0.7980 - accuracy: 0.7867 - auc: 0.9154 - val_loss: 0.5653 - val_precision: 0.8062 - val_recall: 0.7569 - val_fbeta_score: 0.7950 - val_accuracy: 0.7826 - val_auc: 0.9198\n",
      "Epoch 10/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.5730 - precision: 0.8049 - recall: 0.7484 - fbeta_score: 0.7922 - accuracy: 0.7786 - auc: 0.9121 - val_loss: 0.5566 - val_precision: 0.8420 - val_recall: 0.7539 - val_fbeta_score: 0.8211 - val_accuracy: 0.8113 - val_auc: 0.9276\n",
      "Epoch 11/25\n",
      "322/322 [==============================] - 434s 1s/step - loss: 0.5615 - precision: 0.8129 - recall: 0.7564 - fbeta_score: 0.8002 - accuracy: 0.7881 - auc: 0.9160 - val_loss: 0.6688 - val_precision: 0.7663 - val_recall: 0.6983 - val_fbeta_score: 0.7502 - val_accuracy: 0.7461 - val_auc: 0.8815\n",
      "Epoch 12/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.5446 - precision: 0.8162 - recall: 0.7640 - fbeta_score: 0.8046 - accuracy: 0.7930 - auc: 0.9210 - val_loss: 0.6498 - val_precision: 0.7777 - val_recall: 0.6975 - val_fbeta_score: 0.7586 - val_accuracy: 0.7539 - val_auc: 0.8888\n",
      "Epoch 13/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.5502 - precision: 0.8130 - recall: 0.7628 - fbeta_score: 0.8019 - accuracy: 0.7889 - auc: 0.9189 - val_loss: 0.7215 - val_precision: 0.7449 - val_recall: 0.6574 - val_fbeta_score: 0.7233 - val_accuracy: 0.7166 - val_auc: 0.8574\n",
      "Epoch 14/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.5508 - precision: 0.8128 - recall: 0.7556 - fbeta_score: 0.8000 - accuracy: 0.7865 - auc: 0.9186 - val_loss: 0.8047 - val_precision: 0.7031 - val_recall: 0.5895 - val_fbeta_score: 0.6737 - val_accuracy: 0.6770 - val_auc: 0.8209\n",
      "Epoch 15/25\n",
      "322/322 [==============================] - 421s 1s/step - loss: 0.5617 - precision: 0.8119 - recall: 0.7564 - fbeta_score: 0.7994 - accuracy: 0.7883 - auc: 0.9152 - val_loss: 0.7896 - val_precision: 0.7151 - val_recall: 0.5965 - val_fbeta_score: 0.6842 - val_accuracy: 0.6910 - val_auc: 0.8264\n",
      "Epoch 16/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.5567 - precision: 0.8107 - recall: 0.7552 - fbeta_score: 0.7983 - accuracy: 0.7850 - auc: 0.9181 - val_loss: 0.5918 - val_precision: 0.8136 - val_recall: 0.7130 - val_fbeta_score: 0.7892 - val_accuracy: 0.7896 - val_auc: 0.9126\n",
      "Epoch 17/25\n",
      "322/322 [==============================] - 413s 1s/step - loss: 0.5401 - precision: 0.8230 - recall: 0.7630 - fbeta_score: 0.8095 - accuracy: 0.7926 - auc: 0.9217 - val_loss: 0.7618 - val_precision: 0.7263 - val_recall: 0.6157 - val_fbeta_score: 0.6977 - val_accuracy: 0.7026 - val_auc: 0.8396\n",
      "Epoch 18/25\n",
      "322/322 [==============================] - 421s 1s/step - loss: 0.5706 - precision: 0.8043 - recall: 0.7469 - fbeta_score: 0.7914 - accuracy: 0.7784 - auc: 0.9118 - val_loss: 0.6091 - val_precision: 0.8134 - val_recall: 0.7145 - val_fbeta_score: 0.7893 - val_accuracy: 0.7787 - val_auc: 0.9104\n",
      "Epoch 19/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.5652 - precision: 0.8083 - recall: 0.7529 - fbeta_score: 0.7960 - accuracy: 0.7834 - auc: 0.9148 - val_loss: 0.6002 - val_precision: 0.8117 - val_recall: 0.7160 - val_fbeta_score: 0.7885 - val_accuracy: 0.7756 - val_auc: 0.9109\n",
      "Epoch 20/25\n",
      "322/322 [==============================] - 417s 1s/step - loss: 0.5447 - precision: 0.8182 - recall: 0.7566 - fbeta_score: 0.8043 - accuracy: 0.7932 - auc: 0.9204 - val_loss: 0.6797 - val_precision: 0.7679 - val_recall: 0.6713 - val_fbeta_score: 0.7440 - val_accuracy: 0.7469 - val_auc: 0.8752\n",
      "Epoch 21/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.5995 - precision: 0.7965 - recall: 0.7372 - fbeta_score: 0.7830 - accuracy: 0.7712 - auc: 0.9049 - val_loss: 0.6290 - val_precision: 0.8158 - val_recall: 0.7068 - val_fbeta_score: 0.7890 - val_accuracy: 0.7795 - val_auc: 0.9123\n",
      "Epoch 22/25\n",
      "322/322 [==============================] - 415s 1s/step - loss: 0.5463 - precision: 0.8133 - recall: 0.7642 - fbeta_score: 0.8023 - accuracy: 0.7893 - auc: 0.9201 - val_loss: 0.7081 - val_precision: 0.7539 - val_recall: 0.6721 - val_fbeta_score: 0.7341 - val_accuracy: 0.7189 - val_auc: 0.8637\n",
      "Epoch 23/25\n",
      "322/322 [==============================] - 416s 1s/step - loss: 0.5683 - precision: 0.8087 - recall: 0.7566 - fbeta_score: 0.7969 - accuracy: 0.7825 - auc: 0.9144 - val_loss: 0.6930 - val_precision: 0.7601 - val_recall: 0.6821 - val_fbeta_score: 0.7412 - val_accuracy: 0.7329 - val_auc: 0.8687\n",
      "Epoch 24/25\n",
      "322/322 [==============================] - 419s 1s/step - loss: 0.5495 - precision: 0.8128 - recall: 0.7638 - fbeta_score: 0.8019 - accuracy: 0.7916 - auc: 0.9194 - val_loss: 0.8756 - val_precision: 0.6673 - val_recall: 0.5640 - val_fbeta_score: 0.6407 - val_accuracy: 0.6576 - val_auc: 0.7881\n",
      "Epoch 25/25\n",
      "322/322 [==============================] - 435s 1s/step - loss: 0.5473 - precision: 0.8175 - recall: 0.7655 - fbeta_score: 0.8060 - accuracy: 0.7908 - auc: 0.9196 - val_loss: 0.7723 - val_precision: 0.7246 - val_recall: 0.6119 - val_fbeta_score: 0.6956 - val_accuracy: 0.7042 - val_auc: 0.8349\n",
      "training time: 191.07 minutes\n",
      "81/81 [==============================] - 75s 925ms/step - loss: 0.7723 - precision: 0.7246 - recall: 0.6119 - fbeta_score: 0.6956 - accuracy: 0.7042 - auc: 0.8349\n",
      "precision: 72.46%\n",
      "67.65% (+/- 2.43%)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(train_image):\n",
    "    opt=Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=[precision_threshold(threshold = 0.5), \n",
    "                       recall_threshold(threshold = 0.5), \n",
    "                       fbeta_score_threshold(beta=0.5, threshold = 0.5),\n",
    "                      'accuracy','AUC',])\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(train_image,\n",
    "                                                 target_size = (WIDTH,HEIGHT),\n",
    "                                                 batch_size = 16,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(test_image,\n",
    "                                            target_size = (WIDTH,HEIGHT),\n",
    "                                            batch_size = 16,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            shuffle = False)\n",
    "    \n",
    "    start=time.time()\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              validation_data=test_generator,\n",
    "                              epochs=25) \n",
    "    print(\"training time: %.2f minutes\"%((time.time()-start)/60))\n",
    "    \n",
    "    scores = model.evaluate(test_generator)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
